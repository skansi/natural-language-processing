{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Osnovne obrade prirodnog jezika \n",
    "\n",
    "## (NLP, eng. *Natural Language Processing*)\n",
    "\n",
    "[ćeliju izvršavate klikom na nju i zatim Shift+Return, ili samo klikom na ikonicu \"Run cell\"]\n",
    "\n",
    "U ovom ćemo notebooku pokazati kako se računaju osnovne NLP mjere uz pomoć Pythona. Prvi dio se sastoji od ucitavanja oglednog teksta. U ovom slučaju to Moby Dick koji je referenciran kao **mobydick.txt**. Prva linija otvara file i stavlja ga u memoriju na koju pokazuje varijabla **file_opened**, druga linija čita iz memorije i vraća string koji sadrži tekst i sprema je u varijablu **text_content**. Da bi se to dogodilo izvršite ćeliju ispod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_opened = open('mobydick.txt', 'r')\n",
    "text_content = file_opened.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> Kao dodatnu vježbu, modificirajte gornji kod da ispiše cijeli tekst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prvo možemo ispisati broj riječi u fileu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_words = text_content.split()\n",
    "print(len(list_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Možemo sada maknuti sve rijeci duljine 3 znaka ili manje (uočimo da će broj riječi pasti za 40%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_words = [x for x in text_content.split() if len(x)>3]\n",
    "print(len(list_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada ispišemo broj jedinstvenih riječi (svaku različitu brojimo samo jednom):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_words = set(text_content.split())\n",
    "print(len(set_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada potražimo koliko se puta javlja riječ \"whale\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_whale = len([x for x in list_words if x==\"whale\"])\n",
    "print(n_whale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada pogledajmo koliko ima različitih rijeći koje su dulje od 3 slova:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_more_than_3 = len([x for x in set_words if len(x)>3])\n",
    "print(n_more_than_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Kao zadatak, promjenite gornji kod da vam pokaže koliko ima riječi duljih od 10 znakova."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ispisimo sve riječi dulje od 20 znakova:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "more_than_20 = [x for x in set_words if len(x)>20]\n",
    "print(more_than_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kao što vidimo ovdje ima svačega, ba bismo mogli ovo izbaciti iz našeg skupa riječi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "less_than_20_set = [x for x in set_words if len(x)<20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napravimo listu svih bigrama u Moby Dicku i ispišimo prvih 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "bg = bigrams(list_words)\n",
    "bigrami = list(bg)\n",
    "print(bigrami[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Probajte modificirati gornji kod da ispiše sve bigrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sljedeće ćemo pronaći kolokacije. \n",
    "\n",
    "Reimplementirat ćemo algoritam u čistom Pythonu. \n",
    "\n",
    "Krenimo od definicije zajedničke vjerojatnosti. Ako je vjerojatnost događaja $A$ označena kao $P(A)$ i vjerojatnost događaja $B$ kao $P(B)$, vjerojatnost njihovog zajedničke pojave $P(A\\cap B)$ je $P(A)\\cdot P(B)$.\n",
    "\n",
    "Vjerojatnost događaja $A$ se računa pobrojavanjem. Ako je A opisan kao \"Pojava riječi \"whale\" u Moby Dicku\", tada je njena vjerojatnost opisana kao kvocijent broja pojave te riječi (532 puta) i sveukupnog broja riječi (215139 riječi)\n",
    "\n",
    "> Iskoristite Python za izračun ovog kvocijenta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ovdje napisite svoj kod za izracun kvocijenta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Izračunajmo vjerojatnost pojave riječi \"white\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_whale = 532/215139 \n",
    "\n",
    "n_white = len([x for x in list_words if x==\"white\"])\n",
    "\n",
    "p_white = n_white/215139\n",
    "\n",
    "p_whiteAndWhale = p_white * p_whale\n",
    "\n",
    "print(p_whiteAndWhale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sada nadimo broj pojava bigrama \"white whale\" i podijelimo ju s brojem svih bigrama:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dulj_bg = len(bigrami) #uocite da je ovo uvijek duljine n-1 ako je n broj rijeci\n",
    "whiteWhale_lista = [x for x in bigrami if x == ('white','whale')]\n",
    "p_whiteWhale = len(whiteWhale_lista)/dulj_bg\n",
    "print(p_whiteWhale - p_whiteAndWhale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Čime dobijemo razliku između vjerojatnosti da dvije riječi slučajno dođu zajedno i da su one kolokacija. Ovisno o pragu koji postavimo, neki bigram može biti kolokacija ili ne. Standardan prag je 0.01%, što znači da je \"white whale\" ovdje kolokacija.\n",
    "\n",
    "Sa pragom morate eksperimentirati jer će ponekad 0.01% biti previše a nekada premalo. To eksperimentiranje se radi tako da se definira funkcija koja će ovu vrijednost računati za svaki bigram, i onda vratiti sortiranu listu bigrama čija vrijednost prelazi neki prag.\n",
    "\n",
    "To ćemo napraviti sljedeće (sve pokrećemo ispočetka ovdje, tako da je ova ćelija samostalni program):\n",
    "\n",
    "## (samostalni kod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "import time\n",
    "start_time = time.time()\n",
    "file_opened = open('mobydick.txt', 'r')#ovdje umjesto mobydick.txt stavite ime svog filea\n",
    "text_content = file_opened.read().lower()\n",
    "list_words = text_content.split()#[:5000]# obrisite samo prvu povisilicu da biste dobili prvih 5000 rijeci\n",
    "\n",
    "br_rijeci = len(list_words)\n",
    "\n",
    "bg = bigrams(list_words)\n",
    "bigrami = list(bg)\n",
    "\n",
    "def add_scores_to_bigrams(bigram_list, list_of_all_words):\n",
    "    list_bigrams_new = []\n",
    "    for i in bigram_list:\n",
    "        word1 = i[0]\n",
    "        word2 = i[1]\n",
    "        \n",
    "        n_word1 = len([x for x in list_words if x==word1])\n",
    "        n_word2 = len([x for x in list_words if x==word2])\n",
    "        p_word1 = n_word1 / len(list_of_all_words)\n",
    "        p_word2 = n_word2 / len(list_of_all_words)\n",
    "        p_word1andword2 = p_word1 * p_word2\n",
    "        \n",
    "        n_bigram = len([x for x in bigram_list if x == (word1,word2)])\n",
    "        p_bigram = n_bigram/len(bigram_list)\n",
    "        \n",
    "        score = p_bigram - p_word1andword2\n",
    "\n",
    "        trio = [score,[word1,word2]]\n",
    "        #print(trio)\n",
    "        if trio not in list_bigrams_new:\n",
    "            list_bigrams_new.append(trio)\n",
    "        #print(list_bigrams_new[:3])\n",
    "\n",
    "    return sorted(list_bigrams_new,key=lambda x: x[0], reverse=True)\n",
    "\n",
    "\n",
    "   \n",
    "results = add_scores_to_bigrams(bigrami,list_words)#[:20]# obrisite prvu povisilicu da biste limitirali na 20 najznacajnih kolokacija \n",
    "print(\"--- Current runtime is %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "for i in results:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bag of Words (BOW) kreator\n",
    "\n",
    "Ovaj dio notebooka je samostalan i kod koji je dolje je tzv. Bag of Words kreator. Bag of Words je najjednostavniji model za obradu prirodnog jezika. \n",
    "\n",
    "Zamislimo da imamo neki standardni tekst podijeljen na neke fragmente. To može biti na primjer file s 2000 Facebook komentara. Tada svi oni zajedno predstavljanu cijeli *dokument*, a svaki komentar predstavlja jedan *fragment*. Naravno, Facebook komentari mogu sadržavati više od jedne rečenice, pa zato fragment i rečenica nisu jedno te isto, ali ako imamo drugačiji dokument oni to mogu biti. Fragmente definiramo kako želimo, a jedino je važno da su oni relativno mali komadi s obzirom na duljinu dokument. Idealno bi oni trebali tvoriti neke intuitivne cjeline koje imaju smisla (jedan FB komentar je jedna cjelina jer ju je napisao u istom trenutku isti autor), ali je izbor na pojedinom analitičaru da odredi što mu je smisleni fragment.\n",
    "\n",
    "Zamislimo da svaki fragment dodatno ima oznaku sentimenta (ručno označenu) koja je N (negativni) ili P (pozitivni). Ovo tvori jedan jednostavan CSV koji ima retke oblika:\n",
    "\n",
    "> TEXT,SENTIMENT\n",
    "\n",
    "> \"to je to:-)\",P\n",
    "\n",
    "> \"to je bzvz\",N\n",
    "\n",
    "> \"tvoj komentar je bzvz\",N\n",
    "\n",
    "> ...\n",
    "\n",
    "\n",
    "Bag of Words (BOW) model će ovo isto prikazati na drugačiji način. Retci će isto kao i gore predstavljati iste fragmente (istim redom), ali imena stupaca će postati sve riječi koje se javljaju u dokumentu (odnosno koje se javljaju u osnovnom stupcu \"TEXT\"). \n",
    "\n",
    "U određenom smislu, BOW uzima CSV koji ima jedan stupac teksta i možda još neke stupce i iz njega stvara drugi CSV koji prepiše ostale stupce, a supac TEXT zamijeni s N stupaca koji svaki ima za naziv neku riječ iz dokumenta.\n",
    "\n",
    "U retku se zapisuje 0 kao vrijednost stupca \"xyz\" ako taj fragment nema riječ \"xyz\" u sebi. Ako ima i pojavljuje se jednom, zapiše se 1, ako se pojavljuje dva puta zapisuje se 2, itd.\n",
    "\n",
    "Gornji CSV bi kada bi se pretvorio u BOW izgledao ovako:\n",
    "\n",
    "> bzvz,je,komentar,to,tvoj,SENTIMENT\n",
    "\n",
    "> 0,1,0,2,0,P\n",
    "\n",
    "> 1,1,0,1,0,N\n",
    "\n",
    "> 1,1,1,0,1,P\n",
    "\n",
    "\n",
    "Što je konačni izgled CSV-a koji se dobije u outputu. \n",
    "\n",
    "Donja ćelija ce stvoriti iz CSV-a s retcima oblika (TEXT,OZNAKA) novi CSV oblika (riječ1,riječ2,rijec3,..., OZNAKA). Uz notebook imate i testni file koji se zove \"test.csv\" pa možete pogledati u Notepad++ kako on izgleda. Isto tako mora izgledati i Vaš file da ga skripta uspije obraditi i pretvoriti u BOW. Vaš file mora biti u istom direktoriju kao i ovaj notebook da bi stvar radila. Ovaj kod će iz njega napraviti output file u istom direktoriju gdje je i notebook koji će se zvati OUTPUT_FILE.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "in_file = \"test.csv\"\n",
    "out_file = \"OUTPUT_FILE.csv\"\n",
    "\n",
    "\n",
    "df0 = pd.read_csv(in_file, encoding=\"cp1250\")\n",
    "\n",
    "lista =[]\n",
    "for i in df0.iterrows():\n",
    "    lista.append(i[1][0])\n",
    "\n",
    "    \n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "podatci = vectorizer.fit_transform(lista)\n",
    "\n",
    "izvorni_tekst_za_join = np.asarray(lista)\n",
    "row_labels = pd.DataFrame(izvorni_tekst_za_join, columns=['label'])\n",
    "imena_za_cols = np.asarray(vectorizer.get_feature_names())\n",
    "\n",
    "df1 = pd.DataFrame(podatci.toarray(), columns=imena_za_cols)\n",
    "\n",
    "df3 = pd.concat([df1.drop_duplicates(), df0.drop_duplicates()], axis=1)\n",
    "\n",
    "df3 = df3.drop([\"TEXT\"],axis=1)\n",
    "\n",
    "df3.to_csv(out_file, encoding=\"cp1250\", index=False) # utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Možemo ispisati ulazni CSV da vidimo kako izgleda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sada ispišemo izlazni CSV (BOW + oznake sentimenta):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
